{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MS-E2122 - Nonlinear Optimization\n### Prof. Fabricio Oliveira\n \n## Homework 4 - Problem 4.1"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using JuMP\nusing Clp\nusing Random\nusing Test\nusing LinearAlgebra\nusing ForwardDiff\n\n∇(f, x) = ForwardDiff.gradient(f, x)\nd(θ, λ) = ForwardDiff.derivative(θ, λ)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide an implementation of the Armijo line search (which you have implemented in the past, so no need to reimplement)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function armijo(θ; λ=1.0, α=0.01, β=0.7) \n    \n    θ₀  = θ(0)                 # Function value at zero (use \\theta + tab and \\_0 + tab to add the subscript to θ)\n    dθ = d(θ, 0)               # Derivative (slope) at zero   \n    \n    while θ(λ) > θ₀ + α*λ*dθ   # Check termination condition\n        λ = β*λ                # Reduce λ until condition is satisfied\n    end\n    \n    return λ\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4.1 - Frank-Wolfe method\n\nWe use a function that assumes the problem in the form of\n$\n\\begin{align*}\n    \\text{min. }_x & (1/2)||Ax - b||_2^2  \\\\\n    \\text{s.t.: }  & ||x||_1 \\le c  \n\\end{align*}\n$\n\n\nTherefore, the inputs are:\n- Problem instance: matrix $A$, vector $b$ of adequate sizes and a scalar $c$ defining the maximum value for the $L_1$-norm of the vector.\n\n Output\n- f(xᵏ) - optimal objective value \n- xᵏ - optimal solution\n- k - total of iterations."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function frank_wolfe(A, b, c; max_iter=500, ϵ=1e-2)\n\n    # Objective function to be minimized\n    f(x) = 0.5*dot(A*x - b, A*x - b)\n    \n    (m,n) = size(A) # Obtaining problem dimension\n    xᵏ  = zeros(n)  # Initial values set to zeros, as it is always feasible\n    k  = 1          # Iteration counter \n\n    model = Model(optimizer_with_attributes(Clp.Optimizer)) \n    set_silent(model)  # Omits solver output  \n    @variable(model, x[1:n])\n    @variable(model, y[1:n] >= 0)\n    @constraint(model, sum(y[j] for j in 1:n) <= c)\n    @constraint(model, [j=1:n], x[j] <= y[j])\n    @constraint(model, [j=1:n], x[j] >= -y[j])\n    \n    # TODO: Implement the objective function of the linearised problem using xᵏ and ∇(f, xᵏ)\n\n    \n    optimize!(model)  # Optimise the model  \n    x̄ᵏ = value.(x)    # Store x̄ᵏ = argmin{x ∈ ℜⁿ : ∇f(xᵏ)ᵀ(x - xᵏ), x ∈ S}  \n    \n    # TODO: update the improving feasible direction dᵏ\n\n    \n    # TODO: update the new point xᵏ using λ = 1 for the first step and dᵏ\n\n    \n    println(\"Algorithm started. Initial residual: \", round(dot(∇(f, xᵏ), dᵏ), digits=5))\n    while dot(∇(f, xᵏ), dᵏ) > ϵ  && k <= max_iter\n        k  = k + 1 # Iteration counter\n        \n        # TODO: Update the objective function of the linearised problem\n\n        \n        optimize!(model)\n        x̄ᵏ = value.(x)\n\n        # TODO: update the improving feasible direction dᵏ\n\n\n        # Armijo line search (any line search would be ok)\n        θ(λ) = f(xᵏ + λ*dᵏ)\n        λ    = armijo(θ)\n        \n        # TODO: update the new point xᵏ now using λ and dᵏ\n\n\n        if k % 10 == 0 # print every 10 iterations\n            println(\"   residual: \", round(dot(∇(f, xᵏ), dᵏ), digits=5), \" / iter: \", k)\n        end\n    end\n    println(\"Converged. Final residual: \", round(dot(∇(f, xᵏ), dᵏ), digits=5), \" / total iters: \", k)\n    return (f(xᵏ), xᵏ, k) \nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now generate a random instance with 100 features and a 1000 data points. Notice we then call the function to solve the problem."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Generate random data. NOTE: do not rerun\nRandom.seed!(1)\nM   = 1000\nN   = 100\nA   = rand(M,N)\nb   = rand(M)\nc   = 0.8; # sets the desired sparsity level (regularisation coefficient) in the optimal solution."
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Solve model\n(obj, xᵏ, k) = frank_wolfe(A,b,c)\n\n# Print solution\nprintln(\"  Iterations: \", k)\nprintln(\"Optimal cost: \", round(obj, digits = 2))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Testing to check the implementation.\n@test k == 173\n@test obj ≈ 42.93599637615333"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4.2 - Interior point method\n\nWe will implement a primal-dual interior point method to solve a quadratic problem of the form\n\n$\n\\begin{align*}\n\\text{min. } & c^\\top x + \\frac{1}{2}x^\\top Q x \\\\\n\\text{s.t.: } & Ax = b \\\\\n& x \\ge 0.\n\\end{align*}\n$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using JuMP\nusing Clp\nusing Random\nusing LinearAlgebra\nusing ForwardDiff\nusing Plots\npyplot();"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by defining auxiliary function that we will use in the main routine. Notice that you are only required to complete the function `update_newton_direction` which has as inputs\n- The vector $c$, the matrices $Q$ and $A$, and vector $c$;\n- The current primal variable value $x$, dual variables $u$ and $v$;\n- and the penalty term $\\mu$.\n\nThe outputs are the updated directions $d_v, d_u, d_x$.\n\nThe function `calculate_step_size` is used to define a step size to retain feasibility."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\nAuxiliary function: does the update of the Newton direction                             \n\"\"\"\n# Update Newton direction\nfunction update_newton_direction(A, b, c, Q, x, u, v, μ)   \n    # Diagonalize u and x\n    U = Diagonal(u)\n    X = Diagonal(x)\n    e = ones(length(x))\n    \n    # Primal and dual residuals\n    rp = A*x - b\n    rd = A'*v + u - c - Q*x  \n    \n    # TODO: Derive and write the Newton update formulas for each component\n\n\n\n   \n    return (dᵥ,dᵤ,dₓ)\nend\n\n\"\"\"\nAuxiliary function: calculate step size that retains feasibility                             \n\"\"\"\nfunction calculate_step_size(x, d, ϵ)\n    n = length(d)\n    α = 1.0 - ϵ\n    for i = 1:n\n        if d[i] < 0\n            α = min(α, -x[i]/d[i]) # prevents variable becoming negative\n        end\n    end\n    return round(α, digits = Int(-log10(ϵ))) #rounding avoids numerical issues\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main function which is completely provided for you."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n Solve the QP problem and its dual                                          \n\n P: minimize   cᵀx + (1/2)xᵀQx        D: maximize    bᵀv - (1/2)xᵀQx             \n   subject to  Ax = b                    subject to  Aᵀv + u - Qx = c            \n               x ≥ 0                                        u ≥ 0                                        \n\"\"\"\nfunction primal_dual_ip(A, b, c, Q; β=0.3, μ= 10.0, ϵ=1e-6, N=50)\n\n    n = length(c)    # Number of primal variables x and dual variables u\n    m = length(b)    # Number of dual variables v\n    x = zeros(N, n)  # Primal variable x values\n    u = zeros(N, n)  # Dual variable u values\n    v = zeros(N, m)  # Dual variable v values\n    α_p = 1          # Step size for variable x update\n    α_d = 1          # Step size for variable u, and v update\n\n    # Feed an initial solution (cannot use zeros, as it breaks the Newton step due to the inversions of \n    # U and V; any ≠ 0 initial point is good for these x and u. v can be safely initialised as zero)\n    x₀ = ϵ * ones(n)\n    u₀ = x₀\n\n    u[1,:] = u₀\n    x[1,:] = x₀      # Note v₀ initialised as 0\n\n    # Main loop\n    for i = 1:N\n        # Stopping condition #1\n        if n*μ < ϵ           # equivalent to if dot(c,x[i,:]) - dot(b,v[i,:]) < ϵ\n            v = v[1:i,:]\n            u = u[1:i,:]\n            x = x[1:i,:]\n            return (v,u,x)\n        end\n        # Update dₓ, dᵥ, dᵤ\n        (dᵥ,dᵤ,dₓ) = update_newton_direction(A, b, c, Q, x[i,:], u[i,:], v[i,:], μ)\n\n        # Calculate step size α primal and α dual\n        α_p = calculate_step_size(x[i,:], dₓ, ϵ)\n        α_d = calculate_step_size(u[i,:], dᵤ, ϵ)\n\n        # Update variables\n        v[i+1,:] = v[i,:] + α_d*dᵥ\n        u[i+1,:] = u[i,:] + α_d*dᵤ\n        x[i+1,:] = x[i,:] + α_p*dₓ\n\n        # Stopping condition #2\n        if i == N-1\n            return (v,u,x)\n        end\n        # Update μ\n        μ = μ*β\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we provide a numerical example so you can test your code. Notice that the linear constraints are given in the standard form, that is, with additional nonnegative slack variables to convert the inequalities into equalities. \n\nAlthough the original problem has only 2 variables, the final problem has a total of 7, with 5 additional variables for each inequality constraint."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Problem data in standard form.\n# min      -x₁ - x₂ + ⁠(1/2)xᵀQx\n# s.t.: -1/3x₁ + x₂ + x₃                 =  5\n#        1/5x₁ - x₂     + x₄             = -1\n#       -8/3x₁ - x₂         + x₅         = -8\n#        1/2x₁ + x₂             + x₆     =  9\n#           x₁ - x₂ +               + x₇ =  4\n#           x₁, ... ,x₇ ≧  0\n\nRandom.seed!(1)\nc = [-1.0,-1.0, 0.0,0.0,0.0,0.0,0.0]\nb = [ 5.0,-1.0,-8.0,9.0,4.0]\nA = [-1/3  1.0 1.0 0.0 0.0 0.0 0.0;\n      1/5 -1.0 0.0 1.0 0.0 0.0 0.0;\n     -8/3 -1.0 0.0 0.0 1.0 0.0 0.0;\n      1/2  1.0 0.0 0.0 0.0 1.0 0.0;\n      1.0 -1.0 0.0 0.0 0.0 0.0 1.0]\n\n# Create a random positive definite (PD) matrix Q for the quadratic term\nn = length(c)\nQ = randn(n, n)                # Create random matrix\nQ = (Q + Q')/2                 # Make Q symmetric\nif isposdef(Q) == false        # Check if Q is PD\n    λmin = eigmin(Q)           # Minimum eigenvalue\n    Q = Q + (abs(λmin) + 1)*I  # Add λmin + 1 to diagonal elements\nend\nQ[3:n,:] .= 0.0                # Set zero values for slack variables\nQ[:,3:n] .= 0.0\n\n# Solution time (run it twice to calculate the time after everything required is properly compiled)\n(v,u,x) = primal_dual_ip(A, b, c, Q)\n@time (v,u,x) = primal_dual_ip(A, b, c, Q);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a visual representation of the algorithm process. Notice we only plot for $(x_1,x_2)$. Feel free to try different values for the parameters $\\mu$ and $\\beta$ to see how they influence the trajectory and number of steps taken."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "## Plotting\nng = 100\nx1 = range(0, 15, length=ng)\nx2 = x1\n\nplot(x1, 5 .+ (1/3).*x1,  color = :1, label = \"Feas. region\")\nplot!(x1, 1 .+ (1/5).*x1, color = :1, label = \"\")\nplot!(x1, 8 .- (8/3).*x1, color = :1, label = \"\")\nplot!(x1, 9 .- (1/2).*x1, color = :1, label = \"\")\nplot!(x1, -4 .+ x1, color = :1,\n      xaxis  = (\"x1\", (0,10)),\n      yaxis  = (\"x2\", (0,10)),\n      aspect_ratio = :equal,\n      size   = (800,800),\n      label = \"\",\n      title  = \"IPM path\")\n\ng(x) = dot(c[1:2],x) + (1/2)*dot(x,Q[1:2,1:2]*x)\n\ncontour!(x1, x2, (x1, x2) -> g([x1, x2]),\n        levels  = [2, g(x[end,1:2]), 27, 64, 128, 256],\n        clims   = (0,150),\n        clabels = true,\n        cbar = false)\n\ntraj = x[1:end,1:2]\nannotate!(traj[1,1] + 0.2, traj[1,2] + 0.2, text(\"x₀\",9,:bottom))\n\ndisplay(plot!(traj[:,1], traj[:,2], marker = :o, color = :2, label = \"Trajectory\"))\nsavefig(\"qp_convergence.pdf\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4.3 - Sequential quadratric programming\n\nWe will implement a SQP code to solve problems of the form\n\n$\n\\begin{align*}\n    \\text{min. } & f(x) \\\\\n    \\text{s.t.: } & g(x) \\le 0.  \n\\end{align*}\n$\n\nBoth $f: \\reals^s \\rightarrow \\reals$ and $g: \\reals^2 \\rightarrow \\reals$ are assumed to be potentially nonlinear and have a two-dimensional arguments. Notice that there are no equality constraints.\n\nThe function `sqp` has as inputs\n- $f$: the objective function to be minimised\n- $g$: an array of functions $g$ representing the constraints.\n- $\\mu$: penalty term associated with the inequalities\n- $\\Delta$: trust region size, assumed constant in this case.\n- $\\epsilon$: tolerance\n- $N$: maximum number of iterations."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using JuMP\nusing Ipopt\nusing LinearAlgebra\nusing ForwardDiff\nusing Test\nusing Plots\n\n# Gradient and Hessian\n∇(f,x)  = ForwardDiff.gradient(f,x)\n∇²(f,x) = ForwardDiff.hessian(f,x)\n\nfunction sqp(f, g; μ=10.0, Δ=1.0, ϵ=1e-6, N=50)\n    # Initialize parameters\n    k = 1             # Iteration count\n    n = 2             # Number of dimensions of x\n    m = length(g)     # Number of constraints\n    xᵏ = zeros(n)     # Initial primal solution\n    uᵏ = zeros(m)     # Initial dual solution\n    x  = zeros(n, N)  # Save trajectory of iteration\n    x[:,k] = xᵏ       # Set initial value\n   \n    # Defining the Lagrangian function and its Hessian\n    L(x,u) = f(x) + sum(u[i]*g[i](x) for i=1:m)\n    ∇²L(x,u) = ∇²(f,x) + sum(u[i] *∇²(g[i],x) for i=1:m)  # g₂, g₃, and g₄ vanish as they are linear.\n\n    # Main loop\n    for k = 1:N-1\n        xᵏ = x[:,k]\n        \n        # Precompute ∇f, ∇g, and ∇L² for efficiency\n        ∇fᵏ  = ∇(f,xᵏ)\n        ∇gᵏ  = [∇(g[i], xᵏ) for i=1:m]\n        ∇²Lᵏ = ∇²L(xᵏ,uᵏ)\n        gᵏ   = [g[i](xᵏ) for i=1:m]\n\n        # Projected Lagrangian subproblem (Direction search)\n        QP = Model(Ipopt.Optimizer)\n        @variable(QP, d[1:n])\n        @variable(QP, y[1:m] >= 0)\n        \n        # TODO: implement the objective function and constraints in the QP subproblem for the l1-SQP. The code requires that the\n        # linearisation of g is called 'LinearIneq', which requires it to be defined as:\n        # @constraint(QP, LinearIneq[i = 1:m], ... )\n\n\n\n\n\n\n\n\n\n\n        optimize!(QP)\n\n        dᵏ = value.(d)            # Obtain new direction\n        x[:,k+1] = xᵏ + dᵏ        # Update primal solution\n        uᵏ = dual.(LinearIneq)    # Obtain optimal dual solution\n\n        ## Check stopping condition\n        if norm(dᵏ) < ϵ\n            return x = x[:,1:k+1]\n        end\n    end \nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Problem data\n\n# Objective function\nf(x)   = 2x[1]^2 + 2x[2]^2 - 2(x[1]*x[2]) - 4x[1] - 6x[2]\n\n# Constraint functions\ng₁(x) = x[1]^2 - x[2]     # ≦ 0\ng₂(x) = x[1] + 5x[2] - 5  # ≦ 0\ng₃(x) = -x[1]             # ≦ 0\ng₄(x) = -x[2]             # ≦ 0\n\ncons = [g₁, g₂, g₃, g₄]\n\nx = sqp(f,cons)\n# Try different values for μ such as 100.0, or 1000.0. Also, try larger and smaller values for the trust region Δ,\n# such as 0.5 or 100.0."
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing whether the code converged to the right solution. TIP: in case you are having trouble debugging your model, try to to add a prin instruction of the final model (`println(QP)`) AFTER you have added the constraints in the function `sqp()`. The model is printed in a formated form that allows to see which variables and constraints are being created."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@test round.(x[:,end], digits=5) ≈ [0.90499;0.819]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a plot showing the trajectory of the algorithm."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Plotting\nn  = 1000\nx1 = range(-1,2,length=n)\nx2 = x1\n# z  = [f([x1[i],x2[j]]) for j = 1:n, i = 1:n]\n\n# Contours of the objective\ncontour(x1, x2, (x1,x2)->f([x1,x2]),\n        levels  = [-11, -9, -7, -5, -3, -1],\n        clims = (-20,5),\n        clabels = true,\n        legend = false)\n# Plot the feasible region\nplot!(x1, x1.^2, fill = (10,0.2), color = 2)\nplot!(x1, 1 .- (1/5).*x1, fill = (0,0.2), color = 3,\n      xaxis = (\"x1\", (0,2)),\n      yaxis = (\"x2\", (0,2)),\n      aspect_ratio = :equal,\n      size = (800,800),\n      legend = false)\n# Plot the trejectory of iterations\nplot!(x[1,:], x[2,:], marker = :o, color = 1)\n\n# First order approximation at initial point of constraint 1\ndisplay(plot!(x1, -0.25 .+ x1, color = 3, line = :dash))\n\nsavefig(\"SQP_example.pdf\")"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.3"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.3",
      "language": "julia"
    }
  },
  "nbformat": 4
}
